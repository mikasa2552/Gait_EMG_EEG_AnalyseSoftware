# -*- coding: utf-8 -*-

"""Functions to estimate the maximum Lyapunov exponent.

This module provides two functions to estimate the maximum Lyapunov
exponent (MLE) from a scalar and vector time series.

  * mle -- estimate the MLE from a vector time series
  * mle_embed -- estimate the MLE from a scalar time series after
    reconstruction.
"""

from __future__ import absolute_import, division, print_function

import numpy as np
import torch

from . import utils_gpu
from scipy import signal
import os

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å‡å°‘å†…å­˜ç¢ç‰‡
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

def estimate_memory_usage(n, dtype=torch.float64):
    """
    ä¼°è®¡è·ç¦»çŸ©é˜µçš„å†…å­˜ä½¿ç”¨é‡ï¼ˆä»¥ GiB ä¸ºå•ä½ï¼‰ã€‚
    """
    # æ¯ä¸ªå…ƒç´ å ç”¨çš„å­—èŠ‚æ•°
    if dtype == torch.float64:
        bytes_per_element = 8
    elif dtype == torch.float32:
        bytes_per_element = 4
    else:
        raise ValueError("Unsupported dtype")
    
    # è·ç¦»çŸ©é˜µçš„å†…å­˜ä½¿ç”¨é‡
    memory_usage = (n * n * bytes_per_element) / (1024 ** 3)  # è½¬æ¢ä¸º GiB
    return memory_usage

def mle_gpu(y, maxt=500, window=10, metric='euclidean', maxnum=None, batch_size=None):
    """
    è®¡ç®— MLEï¼ˆæœ€å¤§ Lyapunov æŒ‡æ•°ï¼‰ï¼Œæ”¯æŒåˆ†æ‰¹è®¡ç®—è·ç¦»çŸ©é˜µã€‚
    """
    y_tensor = torch.tensor(y, dtype=torch.float64).cuda()
    index, dist = utils_gpu.neighbors_gpu(y_tensor, metric=metric, window=window, maxnum=maxnum)
    m = len(y_tensor)
    maxt = min(m - window - 1, maxt)
    d = torch.empty(maxt, device=y_tensor.device)
    d[0] = torch.mean(torch.log(dist) + 1e-10)

    for t in range(1, maxt):
        t1 = torch.arange(t, m, device=y_tensor.device)
        t2 = index[:-t] + t  # index å·²ç»æ˜¯ PyTorch å¼ é‡
        valid = t2 < m
        t1, t2 = t1[valid], t2[valid]

        if batch_size is None:
            # ç›´æ¥è®¡ç®—è·ç¦»çŸ©é˜µ
            dist_matrix = torch.cdist(y_tensor[t1], y_tensor[t2], p=2)
            dist_diag = dist_matrix.diag()  # æå–å¯¹è§’çº¿å…ƒç´ 
            d[t] = torch.mean(torch.log(dist_diag + 1e-10))
        else:
            # åˆ†æ‰¹è®¡ç®—è·ç¦»çŸ©é˜µ
            dist_diag = []
            for i in range(0, len(t1), batch_size):
                batch_t1 = t1[i:i + batch_size]
                batch_t2 = t2[i:i + batch_size]
                batch_dist_matrix = torch.cdist(y_tensor[batch_t1], y_tensor[batch_t2], p=2)
                dist_diag.append(batch_dist_matrix.diag())
            dist_diag = torch.cat(dist_diag)
            d[t] = torch.mean(torch.log(dist_diag + 1e-10))

        # é‡Šæ”¾æœªä½¿ç”¨çš„å†…å­˜
        torch.cuda.empty_cache()

    return d.cpu().numpy()

# def mle_embed_gpu(x, dim=[1], tau=1, window=10, maxt=500,
#                   metric='euclidean', maxnum=None, parallel=True):
#     """
#     è®¡ç®—ä¸€ç»´æ—¶é—´åºåˆ—çš„ MLEï¼ˆæœ€å¤§ Lyapunov æŒ‡æ•°ï¼‰ï¼ŒåŠ¨æ€é€‰æ‹©æ˜¯å¦ä½¿ç”¨é™é‡‡æ ·å’Œåˆ†æ‰¹è®¡ç®—ã€‚
#     """
#     # é‡æ„ç›¸ç©ºé—´
#     yy = [utils_gpu.reconstruct(x, dim=d, tau=tau) for d in dim]

#     # æ£€æŸ¥æ˜¯å¦éœ€è¦é™é‡‡æ ·å’Œåˆ†æ‰¹è®¡ç®—
#     n = len(yy[0])  # é‡æ„åçš„æ•°æ®ç‚¹æ•°
#     estimated_memory = estimate_memory_usage(n)

#     # è·å– GPU å¯ç”¨å†…å­˜
#     total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # æ€»å†…å­˜ï¼ˆGiBï¼‰
#     allocated_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # å·²åˆ†é…å†…å­˜ï¼ˆGiBï¼‰
#     free_memory = total_memory - allocated_memory  # å¯ç”¨å†…å­˜ï¼ˆGiBï¼‰

#     if estimated_memory > free_memory:
#         print(f"è­¦å‘Šï¼šé¢„è®¡å†…å­˜ä½¿ç”¨é‡ {estimated_memory:.2f} GiB è¶…è¿‡å¯ç”¨å†…å­˜ {free_memory:.2f} GiBï¼Œå¯ç”¨é™é‡‡æ ·å’Œåˆ†æ‰¹è®¡ç®—ã€‚")
#         # é™é‡‡æ ·
#         x_downsampled = signal.decimate(x, q=3)  # é™é‡‡æ ·åˆ° 333 Hz
#         yy = [utils_gpu.reconstruct(x_downsampled, dim=d, tau=tau) for d in dim]
#         # è®¾ç½®åˆ†æ‰¹å¤§å°
#         batch_size = 3000
#     else:
#         batch_size = None

#     # è®¡ç®— MLE
#     if parallel:
#         results = [mle_gpu(y, maxt=maxt, window=window, metric=metric, maxnum=maxnum, batch_size=batch_size) for y in yy]
#     else:
#         results = [mle_gpu(y, maxt=maxt, window=window, metric=metric, maxnum=maxnum, batch_size=batch_size) for y in yy]

#     return np.array(results)
def mle_embed_gpu(x, dim=[3], tau=1, window=10, maxt=500,
                 metric='euclidean', maxnum=50, 
                 max_downsample_factor=10, min_batch_size=100,
                 min_signal_length=500):
    """æœ€ç»ˆç¨³å®šç‰ˆï¼ˆä¿®å¤æ‰€æœ‰å·²çŸ¥é—®é¢˜ï¼‰"""
    # åˆå§‹åŒ–ç¯å¢ƒ
    torch.backends.cuda.matmul.allow_tf32 = True  # å¯ç”¨TensorCoreåŠ é€Ÿ
    torch.backends.cudnn.benchmark = True        # è‡ªåŠ¨ä¼˜åŒ–å·ç§¯ç®—æ³•
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # é˜²ç¢ç‰‡åŒ–

    # é‡æ„ç›¸ç©ºé—´
    x_phase = [utils_gpu.reconstruct(x.copy(), dim=d, tau=tau) for d in dim]

    # æ£€æŸ¥æ˜¯å¦éœ€è¦é™é‡‡æ ·å’Œåˆ†æ‰¹è®¡ç®—
    n_x_phase = len(x_phase[0])  # é‡æ„åçš„æ•°æ®ç‚¹æ•°
    estimated_memory = estimate_memory_usage(n_x_phase)

    # å•æ¬¡æ˜¾å­˜æ£€æµ‹
    total_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    initial_free = torch.cuda.mem_get_info()[0] / (1024**3)
    print(f"[ç³»ç»ŸçŠ¶æ€] æ˜¾å¡æ€»æ˜¾å­˜: {total_mem:.1f}GiB | åˆå§‹å¯ç”¨: {initial_free:.1f}GiB | é¢„è®¡å†…å­˜ä½¿ç”¨é‡ {estimated_memory:.1f} GiB\n")
    print("â”"*40)

    for q in range(1, max_downsample_factor + 1):
        torch.cuda.empty_cache()
        current_free = torch.cuda.mem_get_info()[0] / (1024**3)
        
        try:
            # é™é‡‡æ ·å¤„ç†
            x_processed = signal.decimate(x, q=q, zero_phase=True) if q > 1 else x.copy()
            print(f"[ç­–ç•¥] {'é™é‡‡æ ·' if q>1 else 'åŸå§‹'} q={q} â†’ {len(x_processed)}ç‚¹ ({1000/max(q,1):.1f}Hz)")

            # é‡æ„ç›¸ç©ºé—´
            yy = []
            for d in dim:
                y = utils_gpu.reconstruct(x_processed, dim=d, tau=tau)
                if len(y) < 10:  # å®‰å…¨æ£€æŸ¥
                    raise RuntimeError(f"é‡æ„åæ•°æ®ä¸è¶³10ç‚¹(dim={d})")
                yy.append(y)
            
            # åŠ¨æ€æ‰¹æ¬¡è®¡ç®—
            n = len(yy[0])
            elem_size = 8  # float64=8å­—èŠ‚
            safe_size = int(current_free * 0.8 * (1024**3) / (n * elem_size))
            batch_size = min(n, max(min_batch_size, safe_size))
            
            print(f"[èµ„æº] batch_size={batch_size} (éœ€{round(batch_size*n*elem_size/(1024**3),1)}GiB)")
            

            # æ­£å¼è®¡ç®—
            results = []
            for i, y in enumerate(yy):
                print(f"âŒ› ç»´åº¦{dim[i]}è®¡ç®—ä¸­...", end=' ', flush=True) 
                res = mle_gpu(y, maxt=maxt, window=window, 
                            metric=metric, maxnum=maxnum,
                            batch_size=batch_size)
                results.append(res)

                torch.cuda.synchronize()  # ç¡®ä¿è®¡ç®—å®Œæˆ
                torch.cuda.empty_cache()  # å¼ºåˆ¶é‡Šæ”¾
            print(f"ğŸ‰ q={q}, batch_size={batch_size} æˆåŠŸï¼")
            print("â”"*40)
            return np.array(results)

        except RuntimeError as e:
            err_msg = str(e).split('.')[0]  # å–é”™è¯¯é¦–å¥
            print(f"âš ï¸ q={q}å¤±è´¥: {err_msg}")
            if q == max_downsample_factor:
                raise RuntimeError(f"æ‰€æœ‰é™é‡‡æ ·å°è¯•å¤±è´¥(æœ€å¤§q={max_downsample_factor})")
            continue

    raise RuntimeError("æ„å¤–é€€å‡ºå¾ªç¯")  # ç†è®ºä¸Šä¸åº”æ‰§è¡Œåˆ°æ­¤å¤„